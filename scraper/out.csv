title,url,paragraphs
Build with Linode Hackathon — June 2022,https://townhall.hashnode.com/build-with-linode-hackathon-june-2022,"4 min read,Subscribe to our newsletter and never miss any upcoming articles,We're super excited to announce the Linode Hackathon — Build, deploy, and scale your application easily and cost-effectively in the cloud with Linode.,Embrace yourself, this is going to be awesome! 🎉,Developers choose Linode because it makes managing complex cloud infrastructure easy, with simple bundled pricing, full-featured API, and 100% human support. Linode believes that in order to accelerate innovation in the cloud, virtual computing must be more accessible, affordable, and simple. Its infrastructure-as-a-service platform is deployed across 11 global markets from its data centers around the world and is supported by the Next Generation Network, advanced APIs, comprehensive services, and a vast library of educational resources.,Linode products, services, and people enable developers and businesses to build, deploy, and scale applications more easily and cost-effectively in the cloud. Learn more.,Build an exciting open-source app of your choice using Linode and its products during the whole of June. Publish an article on your Hashnode blog about the process of creating and launching your app for a chance to win one of the 15 cash prizes and swags!,If this is your first time working with Linode, feel free to browse its website and read successful stories of professional users.,This time around, we've got 5 Grand Prizes and 10 Runner Up Prizes on the line.,Projects will be judged based on the following criteria:,If you have any questions please ask them on Hashnode's Discord dedicated server or come talk to us on Twitter at @hashnode.,We can't wait to see your projects! Good luck. ✌,By participating in this hackathon, you agree to the rules laid out here. Please read them carefully before proceeding. All projects submitted in this hackathon should be open source with MIT or other standard open-source licenses.,Hi there 🙆‍♀️, I'm Eleftheria, Community Manager at Hashnode with a coding background and a passion for UX.,Do you have any questions? Don't hesitate to contact me.,I can talk about front-end development, design, job hunting/freelancing/internships.,Hashnode Townhall is the official blog of Hashnode. Read product updates, announcements, interviews, and more in this blog. Get started with Hashnode now.,ARTICLE SERIES,We're super excited to announce the Linode Hackathon — Build, deploy, and scale your application eas…,We are super excited to announce the winners of #HasuraHackathon. A plethora of Hasura open-source a…,Hey Hashnoders 👋, we've decided to continue the strike for the 3rd month in a row with another huge…,We are super excited to announce the winners of #thirdwebhackathon. Several projects were built usin…,Oh boy, we've got a big one for you this time. Brace yourselves:
Hashnode is partnering with the one…,Hey Hashnoders, I hope you are off to a great start in 2022. Hashnode is back with Hackathons and th…,We are super excited to announce the winners of  #Auth0Hackathon. More than 80 projects were built u…,We are super excited to announce the winners of #ClerkHackathon. Several projects were built and dep…,Oh I'm sorry, y'all thought we were done hackin' after last month's blow-out?
Yeah right! Get real!
…,We are super excited to announce the winners of  #HarperDBHackathon. Around 80 projects were built a…,Who is excited about another Hackathon? We certainly are! 🥳
We are super excited to announce a new …,We are super excited to announce a new hackathon on Hashnode in partnership with  HarperDB . 🙌
From…,We are super excited to announce the winners of #AmplifyHashnode Hackathon. Around 80 apps were buil…,It's time, we announce the winners of #VercelHashnode Hackathon,  one of the most successful hackath…,We are super excited to announce the biggest Hackathon on Hashnode till now in partnership with AWS …,We are delighted to announce a new Hackathon on Hashnode in partnership with  Vercel.
Hashnode Hacka…,Hashnode's Christmas Hackathon was one of the most successful developer hackathons during the holida…,Hello Folks! 2020 has been an unusual year for many. But it has also been different in many ways. On…,Syed Fazle Rahman,Co-founder & CEO, Hashnode,Jun 1,Let's go 🚀,Mohit Yadav,I Live to create better apps everyday,Jun 2,Super excited to participate in this hack! Thanks Hashnode and Linode for hosting this awesome hackathon!,Zujaj Misbah Khan,A Passionate Flutter Developer who loves to learn and explore.,Jun 1,There's a typo mistake, shouldn't the winner announcement be 2nd of July?,Syed Fazle Rahman,Jun 1,We've fixed the typo. Thanks for pointing it out. Zujaj Misbah Khan,Zujaj Misbah Khan,Jun 2,Syed Fazle Rahman You're most welcome.,Kunal Keshan,Web Developer. Writer. Creator. On a spree of creative exploration!,Jun 3,Can we participate as a team?,Carlos Powell,Software Developer,Jun 1,Hackathons are back! Thank you, Hashnode. I am definitely participating in this one!,Vikas Rai,Learn -> Practice -> Apply -> Build -> Repeat,Jun 1,Another awesome Hackathon. Thank you organizing such amazing hackathon which motivate us to learn new technology and services.,Ayush Pawar,CS @ IIITG | Documenting My Journey | Flutter Dev,Jun 1,Another month and another great hackathon by Hashnode😍😍. I think the winners would be announced in the second week of July instead of June. A small typo that I noticed. Looking forward to build something cool 👀,Diego Ballesteros (Relatable Code),Senior Software Developer (Relatable Code),Jun 1,Sweet. Looks like a fun hackathon. Going to start brainstorming some ideas later today.,Andrew Baisden,👨🏿‍💻 Software Developer | 📝 Technical Writer | 🖼 Content Creator,Jun 1,Another great Hackathon 🔥🔥🔥,Alemoh Rapheal Baja,Software Developer | Researcher | Writer | Information Professional,Jun 2,Great!!!,Saroj Regmi,Jun 3,Looking forward to the hackathon, But I have a small question is a team submission valid ?,Rutam Prita Mishra,Originally Pull Stack, Certified Full Stack Dev 😉,Jun 3,Hello Syed Fazle Rahman, I am unable to sign up to Linode and get the credits to work using it. The sign-up gets canceled as probably I had an account previously with the same address and cc details for billing. Kindly help me get the credits.,Mostafa Said,I'm a Business Operations Specialist who loves Web Development. Both fields are not so different so I'm passionate to provide value and cont,Jun 4,This will be my first hackathon contribution in Hashnode 🤩 I'm very excited to work with Linode and create something unqiue 🚀,I already won Deepgram hackathon on the Dev Community and it was fantastic experience that I can't wait to have again ❤,Akhil Gautam,Software Developer @bigbinary -- Ruby, JavaScript & Golang enthusiast,Jun 5,Syed Fazle Rahman Eleftheria Batsou Participating in this Hackathon is a problem due to the strict signup process of Linode. My debit card isn't accepted on their portal. It might have happened to others too, but I cannot participate in this Hackathon. Linode should have had some way for people from Hashnode to signup without debit & credit cards.,Anyway ooking forward to the next one! 🎉,Charae Eh Sin,23 hrs ago,Same here. I also currently have a trouble setting up my Linode account due to both of my debit cards are not accepted. Would be really nice if we have other option to temporarily use the platform for this Hackathon.,Lalit,16. A full stack web developer who loves Jamstack,Jun 5,Wow, Hashnode is back with hackathons!!,But this time I will not be able to participate because of school work.,And thank you Hashnode for featuring my thoughts on hackathons :D,All the best to all the participants 👍, waiting to see all the amazing projects.,Anuj varshney,Website Developer,2 hrs ago,I am very super exiceted for partcipatting in the hackathon which as organizing with linode and.,Are You Ready for this amzing hack?,Sign up for a free Linode account and get USD 100 credit 🤑.,Build any open-source project using Linode products and services. Choose your favorite language/tool/framework.,Attribute to Linode and Hashnode from the Project's Readme and the footer of the project.,Launch your app by publishing an article on your Hashnode blog — No blog yet? Set it up now.,Tag the article with #Linode and #LinodeHackathon hashtags! This is how we track who's in. Note: The tag pages are cached and your article will be visible in a few hours.,Share your article on social media - be sure to tag @linode and @hashnode so we can like, retweet, and spread the love!,It's a fun opportunity to earn the recognition you deserve for your skills!,Documenting & sharing what you learn is a great way to cement your knowledge.,You can turn your ideas into reality by using Linode.,You'll be able to get feedback and help from the developer community.,Employers look favorably upon hackathon participants—this is a fantastic way to gain practical experience launching real apps!,1000 USD 💰💰💰💰💰
~= 930 Euros / 77K INR,Hashnode Swag 👕,500 USD 💰💰💰
~= 466 Euros / 39K INR,Hashnode Swag 👕,Hackathon participation badge for all the participants,Product Thinking: Usefulness of the project be in a real-world scenario. Completeness of the features implemented.,UI/UX: The overall look, layout, color usage, and positioning in the application.,Code: Quality (clean code, proper naming conventions, use of linter); use of best practices.,Completeness of the article: The blog should cover all aspects of your project. What inspired you to create this project? What problem does the project address? How did you build it?,Comprehensibility: Try to avoid esoteric jargon in your article. Use simple language to convey your thoughts.,June 1st, 2022 (00:00 PT): Hackathon begins.,June 30st, 2022 (23:59 PT): Deadline for submissions.,2nd Week of July: Winners will be announced."
How I made the most out of my coding bootcamp ?,https://blog.paul-verdure.com/how-i-made-the-most-out-of-my-coding-bootcamp,"Photo by Mars Sector-6 on Unsplash,My coding bootcamp at Wild Code School ended last february after 5 months of intense work, and even if there is still so much to learn, I think starting it was one of the best decisions I ever made. During the course if it, I found out some technics to make the most out of those 5 months, and I wanted to share with you those insights.,When you choose this bootcamp, I suppose you went through some kind of review about its quality, the teacher's qualifications, student satisfaction, first job landing, and so on. You must then have chosen the best possible program according to your profile, objectives, and financial capacities.,Then you can assume that the process works for a majority of people and that it may very well work with you. Every learning process comes with its flaws but in the end, if you stick to it and trust the people from the school, you have all the chances to come out successful from this experience.,When I was studying, I felt sometimes lost and doubtful about whether I could or not become a software developer. Every time I felt like that, I remember the messages I had with an alumnus that was a newbie like me, with no coding experience like me, and who successfully landed a job shortly after the bootcamp. That encouraged me to rely on the school process.,When you learn something new for you, it might be hard sometimes to be self-confident with your capacities, but you can put your trust in the tested learning process to keep moving on in moments of doubts (and there will be some for sure). So stick to the courses, the exercises, the projects, and you will be fine. Of course, you could be more than fine.,If you can manage the time and have the will, I can’t encourage you enough to do some extra work in addition to the classical program.,When I say time, I am not encouraging you to shorten your nights and go sleepless, I even think this is a terrible idea, as your brain needs rest to digest all the new things you learned through the day.,I talk about starting your workday one hour earlier and/or ending it one hour after the last course. It’s already a fair amount of time to do some extra work. This homework can be very diverse:,Training on algorithms and coding challenges on Codewars.,Remake what you have learned the week before on a small project that can be done in a week. Not something to add to your portfolio, but to confirm that you understood the concepts you were taught. Following some short tutorials can help get started but try to do at least some parts yourself, it will help you memorize it. Examples: make a form that sends back a confirmation, make an API request, and display data…,If you want to practice frontend, I can’t recommend enough frontendmentor.com. It gives you all you need to start a cloning project asap, for all levels.,You get it, the goal here is not to build your portfolio (not yet) or showcase your work but to practice and reinforce your learning. Try different types of exercises, tutorials, and projects to find the ones that suit you the best.,I think that doing small projects and exercises during the bootcamp helped me get through it without feeling too overwhelmed. I kept my evenings and weekends free to relax and enjoy my friends and family, but I added one hour or two each day to reinforce my knowledge and practice.,At some point during the session, I remember thinking that I wasn’t going to make it, there was too much to learn and I was under the impression that I wasn’t making much progress. Well, I learn soon after that this feeling was normal and that every student in the course felt the same way at least once.,Whether it last 3, 4, or 6 months, a bootcamp can be exhausting, and your self-confidence can be put to the test pretty seriously by all the new knowledge you have to learn.,At the end of every week, take some time by yourself or with your fellow students to look back at all the stuff you have learned and accomplished since the beginning of the course.,Some say that a coding bootcamp is like a sprint but I prefer to see it as a marathon. To make it to the end, you need to adapt the pace to your mental and physical status, and you must enjoy the breaks. So from time to time, take a notebook and list all the things you learned through the course. You don’t need to master them to write them down, only to have been able to use them once or twice.,If you have the chance I had, you will be guided by great teachers that not only are technically skilled but who will also guide you along the way and until the end of the bootcamp.,But teachers cannot be with each student all the time, and mutual help between students will be key for the class to progress all together. If a student needs help on a subject that you understand at least a little, I strongly encourage you to try and explain him.,Teaching other people is one of the best ways to reinforce your knowledge, as it forces you to explain it with your own words and make this concept your own.,Every time you have the chance to teach, go on! Some students will be too shy or won’t dare to ask for help. So if you feel confident enough, offer to explain to whoever needs it, and don’t worry, no one will expect you to be as good as the class teacher but they will appreciate your assistance.,The day I write those lines, it has been 3 months since my coding bootcamp ended. What’s left of it now?,The knowledge? It is now digested and well surpassed thanks to my first weeks as a junior developer in the company where I work.,The projects I built? Well, that goes with the knowledge, as my competencies are evolving, I am planning to start new projects I will try to build thanks to these fresh new abilities. Not that I am not proud of the group work we accomplish, I am, truly. But I am not the developer I was at the end of the program, and I want to showcase what I learned to future recruiters.,What stayed with me most after this 5-month journey are the people I met along the ride. Some are my friends now and we discuss almost every day our successes, our struggles, our hesitations… These connections are truly precious and they make me feel much less lonely in the aftermath of the bootcamp. We support each other in every step and this is so precious.,So, even if there are tensions and not everyone can be friends, try to be open to new friendships and keep an open heart to your co-students.,A coding bootcamp is a professional goal but it is also a human adventure and I encourage you to enjoy it for this part as well. It will make your days sweeter if you attend your courses with some friends.,To sum it up, here are my pieces of advice to help you make the most out of your bootcamp:,You choose that bootcamp for a reason, so trust the school and the process that helped numbers of students get a first job as software engineer.,Do some extra work on small, easily measurable exercises/projects to reinforce what you learned during the week. Do push it far through the night, you need to rest and stay fresh.,From time to time, look at all the work you've done since the beginning and congrtaulate yourself.,If you understand a concept that other students don’t, offer your help, it’s a great way to learn. And if you are in need, ask for help, others will be happy to teach you.,Don’t just focus on learning, but socialize also and start to build your first connections in this new industry.,I hope you enjoyed this article. My name is Paul, and I am a Front-End Software Engineer. For more articles like that, you can follow me on Twitter.,See you next time!,From Craft Brewer & Entrepreneur to Software Developer @tudigo in Bordeaux, France. Love to build projects and solve problems in JS & CSS. Enjoying every minute of this journey.,Andrew Baisden,👨🏿‍💻 Software Developer | 📝 Technical Writer | 🖼 Content Creator,May 27,So many great points this had so much good info Paul Verdure thanks for sharing.,Paul Verdure,May 27,Thanks Andrew Baisden for your feedback. I really appreciate.,Eleftheria Batsou,Community Manager at Hashnode | Developer & UX | Youtuber ❤,May 27,Great work Paul Verdure! 👏👏,Paul Verdure,May 27,Thanks a lot Eleftheria Batsou,CESS,currently learning JavaScript 💙 | self-taught👩🏻‍💻 | Frontend Developer,May 27,Thanks for sharing,Paul Verdure,May 27,My pleasure CESS!,Mudit Mishra,Web developer | Technical Writer,May 27,Can totally relate to this💯. I'm going to send this to my friend going through this. It's really helpful🙌 Thanks for sharing!,Paul Verdure,May 27,Thanks for your nice comment Mudit Mishra !,Egokiphovwen Godstime Okiemute,A Front-End Developer,May 27,Enjoyed this post Because currently I'm going through a bootcamp,We're just a week into it and this will help me a lot,Paul Verdure,May 28,Glad you liked it Egokiphovwen Godstime Okiemute. Have a great bootcamp!,Quadri Borokinni,Quadri Borokinni is a Statistician and Data analyst having a strong theoretical and Practical knowledge of data analysis with various statis,May 28,Thanks for sharing these tips,Paul Verdure,May 29,Happy you liked it Quadri Borokinni,Hello Hello,May 29,Thank for sharing! 👏👏,Paul Verdure,May 29,You are very welcome Hello Hello,Chibuzo Odigbo,I am here to make a large digital web cake, and delve into the nits of the web to discover more!,May 30,Thank you for this bit Paul!!! I will try to work by it!,Paul Verdure,May 30,My pleasure Chibuzo Odigbo,Daniel Hawaliz,💥 Ideation | Design | Code | eCommerce 💥,May 31,Awesome Article Paul Verdure. What was your main reason to even start the coding bootcamp?,Paul Verdure,May 31,Thanks Daniel Hawaliz. The truth is, I love learning new things, and this industry is just a nerver-ending learning path. A bootcamp was perfect for me to grasp to basic concepts and being able now to learn by myself.,aas pass,May 31,Thanks for sharing,Paul Verdure,May 31,You are very welcome aas pass!,Roger Mullins,Programmer | Historian | Writer,Jun 4,Excellent article and very timely for me as my bootcamp starts in August. Thank you for sharing your insights!,Trust the process,Do some extra homework,Look back once in a while,Help and explain,Connect with other students,Wrapping Up,Training on algorithms and coding challenges on Codewars.,Remake what you have learned the week before on a small project that can be done in a week. Not something to add to your portfolio, but to confirm that you understood the concepts you were taught. Following some short tutorials can help get started but try to do at least some parts yourself, it will help you memorize it. Examples: make a form that sends back a confirmation, make an API request, and display data…,If you want to practice frontend, I can’t recommend enough frontendmentor.com. It gives you all you need to start a cloning project asap, for all levels.,You choose that bootcamp for a reason, so trust the school and the process that helped numbers of students get a first job as software engineer.,Do some extra work on small, easily measurable exercises/projects to reinforce what you learned during the week. Do push it far through the night, you need to rest and stay fresh.,From time to time, look at all the work you've done since the beginning and congrtaulate yourself.,If you understand a concept that other students don’t, offer your help, it’s a great way to learn. And if you are in need, ask for help, others will be happy to teach you.,Don’t just focus on learning, but socialize also and start to build your first connections in this new industry."
The Best Document Similarity Algorithm in 2020: A Beginner's Guide | by Masatoshi Nishimura | Medium | Towards Data Science,https://towardsdatascience.com/the-best-document-similarity-algorithm-in-2020-a-beginners-guide-a01b9ef8cf05,"Aug 26, 2020,Save,If you want to know the best algorithm on document similarity task in 2020, you’ve come to the right place.,With 33,914 New York Times articles, I’ve tested 5 popular algorithms for the quality of document similarity. They range from a traditional statistical approach to a modern deep learning approach.,Each implementation is less than 50 lines of code. And all models being used are taken from the Internet. So you will be able to use it out of the box with no prior data science knowledge, while expecting a similar result.,In this post, you’ll learn how to implement each algorithm and how the best one is chosen. The agenda goes by:,You want to dip your toes into natural language processing and AI. You want to spice up the user experience with relevant suggestions. You want to upgrade the old existing algorithm. Then you will love this post.,Shall we get started?,You decide to search for the term “best document similarity algorithms”.,Then you will get search results from academic papers, blogs, to q&a. Some focus on a tutorial of a specific algorithm, while the other focuses on a theoretical overview.,In academic papers, a headline says this algorithm performed 80% accuracy beating all the others that achieved only 75%. Ok. But will that difference enough to make it noticeable in our eyes? What about 2% increase? How easy is it to implement that algorithm? Scientists have a bias towards going for the best in a given test set leaving out the practical implication.,In Q&A, hyped enthusiasts dominate the conversation. Some say the best algorithm today is BERT. That algorithm concept is so revolutionary it beats everything else. The cynics, on the other hand, call everything depends on the job. Some answers are from years ago predating deep learning. Take a look at this Stackoverflow. When the most voted was written in 2012, it’s hard to judge what it truly means to us.,Google would be happy to throw in millions of dollars in engineer power and the latest computational power just to improve their search 1% better. That will not likely be practical nor meaningful for us.,What is the trade off between the performance gain and the technical expertise required for implementation? How much memory does it require? How fast does it run with minimal preprocessing?,What you want to see is how one algorithm is better than another in a practical sense.,This post will provide you with a guideline as to which algorithm to implement for your next document similarity problem.,There are 4 goals in this experiment:,“Pre-trained models are your friend.“,- Cathal Horan, machine learning engineer at Intercom,For this experiment, 33,914 New York Times articles are selected. They are from 2018 to June 2020. The data has been mostly collected from the RSS feed that is parsed with full content. An average length of articles is 6,500 characters.,From the pool, we choose 5 as the basis for similarity search. Each represents a different category.,On top of the semantic categories, we will measure written formats as well. The more description is down below.,We will use 5 criteria to judge the nature of similarities. Please skip this section if you just want to see the results.,Tags are the closest proxy to human judgments in content similarity. Journalists themselves write down the tags by hand. You can inspect them at news_keywords meta tags in the HTML headers. The best part of using tags is that we can objectively measure how much overlap the two contents have. Each tag goes in size ranging from 1 to 12. The more overlap 2 articles have, the more similar they are.,Second, we look at the section. That’s how New York Times categorizes articles at the highest level: science, politics, sports, etc. The first part of URL displays section (or slug) right after the domain (nytimes.com/…).,The second is a subsection. For example, an opinion section can be subdivided into world, or world into Australia. Not all articles contain it, and it is not as significant as the other two.,The fourth is the style of writing. Most analysis of document comparison only looks at the semantics. But since we are comparing the recommendation in a practical use case, we want similar writing styles as well. For example, you do not want to get a commercially focused reading about “the top 10 running shoes” right after the “running shoes and orthotics” from an academic journal. We will group articles based on the writing guidelines taught at Jefferson County Schools. The list follows human interest, personality, the best (ex: product review), news, how-to, past events, and informational.,These were the algorithms we will look at.,Each algorithm was run against 33,914 articles to find the top 3 articles with the highest scores. That process is repeated for each of the base articles.,The input was the article content in full length. Titles were ignored.,Be aware, some algorithms are not built for document similarity in mind. But with such diverse opinions on the internet, we shall see the outcome with our own eyes.,We will not focus on conceptual understanding nor on a detailed code review. Rather the aim is to demonstrate how easy the setup is. Don’t worry if you do not understand every detail explained here. It’s not important to follow along the post. For understanding the theory, check the reading list at the bottom for the excellent blogs written by others.,You can find the entire codebase in the Github repo.,If you just want to see the results, skip this section.,Paul Jaccard proposed this formula over a century ago. And the concept has been the standard go-to for similarity tasks for a long time.,Luckily, you will find the jaccard the easiest algorithm to understand. The math is straightforward with no vectorization. And it lets you write codes from scratch.,Also, jaccard is one of the few algorithms that does not use cosine similarity. It tokenizes the words and calculates the intersection over union.,We use NLTK to preprocess the text.,Steps:,This is another well established algorithm that has been around since 1972. With enough battle testing for decades, it is the default search implementation of Elasticsearch.,Scikit-learn offers nice out of the box implementation of TF-IDF. TfidfVectorizer lets anyone try this in a blink of eyes.,The results of TF-IDF word vectors are calculated by scikit-learn’s cosine similarity. We will be using this cosine similarity for the rest of the examples. Cosine similarity is such an important concept used in many machine learning tasks, it might be worth your time to familiarize yourself (academic overview).,Thanks to scikit-learn, this algorithm came out with the shortest lines of codes.,Word2vec came out in 2014, which took the breath of developers at the time. You might have heard of the famous demonstration:,King - Man = Queen,Word2vec is great at understanding the individual word, vectorizing the whole sentence takes a long time. Let alone the entire document.,Instead we will use Doc2vec — a similar embedding algorithm that vectorizes paragraphs instead of each word (2014, Google Inc). In a more digestible format, you can check out this intro blog by Gidi Shperber.,Unfortunately for Doc2vec, no corporation sponsored pretrained model has been published. We will use pretrained enwiki_dbow model from this repo. It is trained on English Wikipedia (unspecified number but the model size is decent at 1.5gb).,The official documentation for Doc2vec states you can insert any amount of input however long. Once tokenized, we will feed in the entire document, using gensim library.,This is a popular algorithm published by Google much more recently in May 2018 (the famous Ray Kurzweil was behind this publication🙃). The implementation detail is well documented in Google’s Tensorflow.,We will use the latest official pretrained model by Google: Universal Sentence Encoder 4.,As the name suggests, it’s been built with a sentence in mind. But the official document does not constrain the input size. There’s nothing that’s stopping us from using it for a document comparison task.,The whole document is inserted into Tensorflow as is. No tokenization is done.,This is a big shot. Google open sourced BERT algorithm in November 2018. In the following year, Google’s vice president in Search published a blog post calling BERT their biggest leap in the past 5 years.,It is specifically built to understand your search query. When it comes to understanding the context of one sentence, BERT seems to outperform every other mentioned here.,The original BERT task was not meant to handle a large amount of text input. For embedding multiple sentences, we will use Sentence Transformers open source project published by UKPLab (from German University), whose calculation speed is superior. They also provide us with a pretrained model as well that is comparable to the original model.,So each document is tokenized into sentences. And the results are averaged to represent the document in one vector.,Let’s see how each algorithm performs in our 5 different types of articles. We select the top 3 articles by the highest scores for comparison.,In this blog post, we will go through only the results of the best performing algorithm for each of the five. For the full results along with individual article links, please see the algorithm directories in the repo.,BERT wins.,The article is a human interest story that involves a romantic date for a divorced woman in 50s.,This style of writing does not carry specific nouns like celebrity names. It is not time-sensitive either. One human interest story from 2010 would be likely as relevant today. Therefore, not one algorithm was far off in the comparisons.,It was a close call against USE. While a USE story took a detour into a social issue such as LGBTQ, BERT focused solely on romance and dating. Other algorithms diverged into the topics on family and children, possibly from seeing the word “ex husband”.,TF-IDF wins.,This scientific article talks about 3D scanning the active volcano inside the ocean.,3D scanning, volcano, and ocean are rare terminologies and they are easy to pick up. All algorithms faired well.,TF-IDF correctly picked those that only talk about volcano within the earth’s ocean. USE was a good contender as well with the focus on volcanoes on Mars instead of oceans. Others picked up articles about Russian military submarines which is not science-related and off topic.,TF-IDF wins.,The article talks about what has happened to Renault and Nissan after the former CEO Carlos Ghosn escaped.,The ideal matches would talk about those 3 entities. Compared to the previous two, this article is much more event driven and time-sensitive. The relevant news should happen similar to this date or after (it is from November 2019).,TF-IDF correctly chose articles that focus on Nissan CEO. Others picked articles that were talking about generic automotive industry news such as an alliance between Fiat Chrysler and Peugeot.,It’s also worth mentioning Doc2vec and USE led to exactly the same result.,Tie among Jaccard, TF-IDF and USE.,The article is about tennis player Dominic Thiem in Australian Open (tennis match) in 2020.,The news is event driven and very specific to the individual. So ideally the matches will be about Dominic and Australian Open.,Unfortunately, the result suffered from a lack of sufficient data. All of them talked about tennis. But some of the matches were talking about Dominic in French Open from 2018. Or, did about Roger Federer from the Australian Open.,The result is a tie among the 3 algorithms. This speaks of the crucial importance: we need to our best at collecting, diversifying, and expanding the data pool for the best similarity matching result.,USE wins.,The article is about Democrat with a particular focus on Bernie Sanders appearing on Fox News for the 2020 election.,Each topic can be big on its own. There is an abundance of articles about democratic party candidates and the election. Since the gist of the story is a novelty, we prioritized those that discuss the relation of democratic party candidate and Fox.,A side note: in practice, you want to be careful with suggestions in politics. Mixing liberal and conservative news can easily upset readers. Since we are dealing with New York Times alone, it will not be our concern.,USE found articles that talked about Bernie Sanders and TV cables such as Fox and MSNBC. Others picked articles that discuss other Democrat candidates in 2020 election. Those were considered too generic.,Before concluding the winner, we need to talk about the performance time. Each algorithm performed very differently in terms of speed.,The result was that TF-IDF implementation was way faster than any other. To calculate through 33,914 documents on a single CPU from the start to end (tokenization, vectorization, and comparison), it took:,TF-IDF took only one and a half minutes. That’s 2.5% of what it took on USE. Of course, you can incorporate multiple efficiency enhancements. But the potential gain needs to be considered discussed first. It would give a reason us another reason to have a hard look at the development difficulty trade-off.,From the result, we can say for a documentation similarity in news articles, TF-IDF is the best candidate. That’s particularly true if you use it with minimal customization. It is also surprising given TF-IDF is the second oldest algorithm invented. Rather you might be disappointed that the modern state-of-art AI deep learning does not mean anything in this task.,Of course, each deep learning technique can be improved by training your own model and preprocess the data better. But all come with development costs. You want to be thinking hard about how better that effort will bring in relative to the naive TF-IDF method.,Lastly, it is fair to say we should forget about Jaccard and Doc2vec altogether in document similarity. They do not bring any benefit compared to the alternative today.,Say you are deciding to implement the similarity algorithm in your application from scratch, here is my recommendation.,The state of art in document similarity match out of the box is TF-IDF, despite the deep learning hype. It gives you a high-quality result. And the best of all, it is lightning fast.,As we’ve seen, upgrading it to deep learning methods might or might not give you a better performance. Lots of thoughts must be placed beforehand to calculate the trade-off.,Andrew Ng gave an analogy “data is the new oil” back in 2017. You cant expect your car to run without oil. And that oil has to be good.,Document similarity relies as much on the data diversity as on the specific algorithm. You should put most of your effort to find unique data to enhance your similarity result.,Only and only if you are dissatisfied with the result of TF-IDF, migrate to USE or BERT. Set up the data pipeline and upgrade your infrastructure. You will need to take into account the explosive calculation time. You will likely preprocess the word embedding, so you can handle similarity matching at a run time much faster. Google wrote a tutorial on that topic.,You can slowly upgrade your model. Training your own model, fitting the pretrained into the specific domain, etc. There are also many different deep learning models available today. You can try one by one to see which one fits your specific requirement the most.,You can achieve document similarities with various algorithms: some are traditional statistical approach and others are cutting edge deep learning methods. We have seen how they compare to one another in the real world New York Times articles.,With TF-IDF, you can easily start your own document similarity on your local laptop. No fancy GPU is necessary. No large memory is necessary. With high-quality data, you will still get competitive results.,Granted, if you want to do other tasks such as sentiment analysis or classification, deep learning should suit your job. But, while researchers try to push the boundary of deep learning efficiency and performance, it’s not healthy for all of us to live in the hype loop. It creates tremendous anxiety and insecurity for the newcomers.,Staying empirical can keep our eyes on reality.,Hopefully, the blog has encouraged you to start your own NLP project.,Let’s start getting our hands dirty.,This post is made in conjunction with the document clustering of Kaffae project I’m working on. Along that line, I am planning to do another series on a sentence similarity.,Stay tuned😃,Defining the best;,Experiment goal statement;,Data setup;,Comparison criteria;,Algorithm setup;,Picking the winner;,Suggestion for starters;,By running multiple algorithms on the same dataset, you will see which algorithm fairs against another and by how much.,By using full-length articles from popular media as our dataset, you will discover the effectiveness of real-world applications.,By accessing article URLs, you will be able to compare the differences in result quality.,By only using the pretrained models available publicly, you will be able to set up your own document similarity and expect similar output.,How My Worst Date Ever Became My Best (Lifestyle, Human Interest),A Deep-Sea Magma Monster Gets a Body Scan (Science, Informational),Renault and Nissan Try a New Way After Years When Carlos Ghosn Ruled (Business, News),Dominic Thiem Beats Rafael Nadal in Australian Open Quarterfinal (Sports, News),2020 Democrats Seek Voters in an Unusual Spot: Fox News (Politics, News),Tag Overlap,Section,Subsections,Story Style,Theme,Jaccard,TF-IDF,Doc2vec,USE,BERT,Lowercase all text,Tokenize,Remove stop words,Remove punctuation,Lemmatize,Calculate intersection/union in 2 documents,TF-IDF: 1.5 min.,Jaccard: 13 min.,Doc2vec: 43 min.,USE: 62 min.,BERT: 50+ hours (each sentence was vectorized).,BERT,TF-IDF,TF-IDF,Tie among Jaccard, TF-IDF, and USE,USE,An article covering TF-IDF and Cosine similarity with examples: “Overview of Text Similarity Metrics in Python”.,An academic paper discussing how cosine similarity is used in various NLP machine learning tasks: “Cosine Similarity”.,Discussion of sentence similarity in different algorithms: “Text Similarities : Estimate the degree of similarity between two texts”.,An examination of various deep learning models in text analysis: “When Not to Choose the Best NLP Model”.,Conceptual dive into BERT model: “A review of BERT based models”.,A literature review on document embeddings: “Document Embedding Techniques”"
Semantic Search with S-BERT is all you need | by Subir Verma | MLearning.ai | Medium,https://medium.com/mlearning-ai/semantic-search-with-s-bert-is-all-you-need-951bc710e160,"Jun 5, 2021,Save,Bloomberg - Semantic search is a data searching technique in which a search query aims to not only find keywords but to determine the intent and contextual meaning of the words a person is using for a search.,Semantics refers to the philosophical study of meaning. It’s true that philosophy rarely rhymes with software engineering, but this concept does help us reach a definition. Indeed, Semantic Search is related to figuring out what your user means.,Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines, which only find documents based on lexical matches, semantic search can also find synonyms.,In fact, this type of search makes browsing more complete by understanding almost exactly what the user is trying to ask, instead of simply matching keywords to pages. The idea behind semantic search is to embed all entries in your corpus, which can be sentences, paragraphs, or documents, into a vector space.,At search time, the query is embedded into the same vector space and the closest embedding from your corpus is found. These entries should have a high semantic overlap with the query.,FYI — For this blog, I am sticking with variants of Sentence-Transformers. SentenceTransformers is designed in such a way that fine-tuning your own sentence/text embeddings models is easy.
It provides most of the building blocks that you can stick together to tune embeddings for your specific task.
If you are not aware of this concept please do read their publications here: https://www.sbert.net/docs/publications.html,Before we begin let’s ask ourselves these questions and look for its solution rather than directly jumping into solution:,subirverma.medium.com,Q1. What sort of embeddings will work ?
Q2. How to store documents and their huge embeddings if using BERT?
Q3. What if we have long documents like blogs and small pieces of content like product descriptions? How will the approach change?
Q4. How model fine-tuning can give me good results ?,Let’s see if we can get answers to all of our questions. While we do that following concepts will be covered:
1. Search Types
2.Cosine and Dot product metric
3. Document Embedding techniques
4. FAISS storage and retrieval.
5. Synthetic Query Generation.
6. Bi-Encoder fine-tuning,I have shared a detailed analysis of this blog over a video here.,The solution to Q1 & Q3
A critical distinction for your setup is symmetric vs. asymmetric semantic search:,The above concept is very important as this will help us understand our problem better and thus we can work on using methods developed specifically for the task/problem which we have in our hand.,It is critical that you choose the right model for your type of task. It is mostly distinguished by the type of data it has been trained on. Also, models tuned for cosine-similarity will prefer the retrieval of short documents, while models tuned for dot-product will prefer the retrieval of longer documents. Depending on your task, the models of one or the other type are preferable.,Suitable models for symmetric semantic search:,Suitable models for asymmetric semantic search:,More details here: https://www.sbert.net/docs/pretrained_models.html,In order to understand why S-BERT not BERT i highly recommend the paper here: https://arxiv.org/pdf/1908.10084.pdf,Solution to Q2
At this point, we have an understanding of our data and accordingly, we have selected the embeddings model. Now next we need to understand how to encode data and what other information we need to store with encodings will be helpful in retrieving search results.,For storage, we have options like :
(a) ElasticSearch: It can be a very good option if we have a lot of meta-information storage and we want to run some cross-cluster search. Having said that it can cost you a lot and maintenance can cost you another expert if shipping things to production and scale.
(b) FAISS: (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves the limitations of traditional query search engines that are optimized for hash-based searches and provides more scalable similarity search functions.
(c ) Annoy: a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mapped into memory so that many processes may share the same data.,It is completely up to you and your requirements what you choose. I choose FAISS for now, for the ease of use (pythonic) and GPU support makes is blazing fast.
The only limitation with FAISS is you have to maintain your meta-data information separately in some DataBase with mapping of your FAISS ids.
Read here: https://medium.com/swlh/add-similarity-search-to-dynamodb-with-faiss-c68eb6a48b08,DataSet: Wikipedia Movie Plots
Content: The dataset contains descriptions of 34,886 movies from around the world. Column descriptions are listed below:,We want to build a search bar for users to search for movies and for simplicity let us consider we want to search in the movie plot field only. Users are allowed to type in some keywords or sentence which can describe their movie and we bring the best we can by closely understanding the Query and Movie plots.,Before encoding, let’s see how long this text information is.,There are a couple of ways to handle these scenarios where document length is more than 512.
(1) Easiest and Scariest are to slice everything off for a length greater than 512.
(2) Run Extractive or Abstractive Summarisations
(3) Average of Document pool embeddings.,Right now we will choose the scariest one. I went through some samples of the plot and concluded that taking max length should suffice what we are trying to build with movie search.
Here, as discussed above, we are dealing with Asymmetric Search and trying to retrieve long passages so for our case, a dot-product model will suit well. And for this experiment, i am choosing “sentence-transformers/msmarco-distilbert-base-dot-prod-v3” model which performs great in Semantic Textual Similarity (Asymmetric ) tasks and it’s quite faster than BERT as it is considerably smaller. This model was optimized to be used with dot-product as a similarity function between queries and documents.
Note: If you have short descriptions, “distilbert-base-nli-stsb-mean-tokens”, works a lot better and faster.,Since we are using FAISS, storing embeddings is easy, once we have settled with Index Factory and Mappings.,We have encoded our Movie Plot, where each plot has been encoded with a 768-dimensional vector and stored to disk with movie_plot.index name.
Note here we have used index.add_with_ids and this encodes data in the order of data-frame and stores their index ids too.,Let’s write a couple of utility functions which will help encode user query and fetch similar movies from FAISS index directory.,Function: search
query: string -> user typed query
top_k: integer -> number of results to be returned
index: faiss_index -> index to query the for results
model: sbert -> model to encode the user-query
Function: fetch_movie_info
dataframe_idx: integer -> index value extracted from movie_plot.index which can be used to map back to our data-frame to fetch additional information.,I tried running few queries, very vague and I got some decent results. Not impressed but compared to how just by narrowing down the task it has been trained and fine-tuned and then relating back to our use case gave us some good results.,Solution to Q4
We could have easily fine-tuned a sentence-transformer model on our dataset given if we had query & relevant passages information. But you would not have this data if building something from ground zero. (Here we are not dealing with pre-training approaches of transformer models. It is expensive and requires a huge deal of data. Not to forget the domain here ),At this point, we got nothing. But wait, what about all the movie plots textual information we have. Can we devise some unsupervised approach to fine-tune our model on our dataset?,We use synthetic query generation to achieve our goal. We start with the passage from our document collection and create for these possible queries users might ask / might search for.
BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models presented a method to learn (or adapt) model for asymmetric semantic search without requiring training data.,Paper: https://arxiv.org/abs/2104.08663
GitHub: https://github.com/UKPLab/beir,This paper has some amazing ideas on Multi-Task Learning and fine-tuning.
From the paper
GenQ Similar to past work (Liang et al., 2020; Ma et al., 2021), we propose an unsupervised domain-adaption approach for dense retrieval models using synthetic queries. First, we fine-tune a T5-base (Raffel et al., 2020) model to generate queries given a passage. We use the MSMARCO dataset and train for 2 epochs. Then, for a target corpus we generate 5 queries for each document using a combination of top-k and nucleus-sampling (top-k: 25; top-p: 0.95). Due to resource constraints, we cap the maximum number of target documents in each dataset to 100K. We found the T5 model to perform better compared to BART (Lewis et al., 2020) and our decoding setting better as compared to beam-search. For retrieval, we continue to fine-tune the SBERT model (section 4.1.3) on the synthetic queries and document pairs. Note, this creates an independent model for each task.,Inspired by the paper, we will use a similar fine-tune technique for SBERT. Let’s see the synthetic query generation code first.,The paragraphs input to this code are nothing but chunks of the movie plot and each chunk will have a maximum of 5 synthetically generated queries. If you think for a moment what are we trying to do here is have the possible information present in paragraphs represented as questions and then use this knowledge tuple to fine-tune a s-bert model which will capture the semantic and syntactic information mapping between these tuples.,SBERT is a siamese bi-encoder using mean pooling for encoding and cosine-similarity for retrieval.
SentenceTransformers was designed in such a way that fine-tuning your own sentence/text embeddings models is easy. It provides most of the building blocks that we can stick together to tune embeddings for our specific task.
we can create the networks architectures from scratch by defining the individual layers. For example, the following code would create the depicted network architecture:,Note: Here I am using the same “sentence-transformers/msmarco-distilbert-base-dot-prod-v3” for fine-tuning which I used in the first half. No changes there.
I want here to talk about the loss function which we will be using. sentence_transformers.losses define different loss functions, that can be used to fine-tune the network on training data. The loss function plays a critical role when fine-tuning the model. It determines how well our embedding model will work for the specific downstream task. Sadly there is no “one size fits all” loss function. Which loss function is suitable depends on the available training data and on the target task.,This loss expects as input a batch consisting of sentence pairs (a_1, p_1), (a_2, p_2)…, (a_n, p_n) where we assume that (a_i, p_i) are a positive pair and (a_i, p_j) for i!=j a negative pair. For each a_i, it uses all other p_j as negative samples, i.e., for a_i, we have 1 positive example (p_i) and n-1 negative examples (p_j). It then minimizes the negative log-likelihood for softmax normalized scores. This loss function works great to train embeddings for retrieval setups where you have positive pairs (e.g. (query, relevant_doc)) as it will sample in each batch n-1 negative docs randomly. The performance usually increases with increasing batch sizes. You can also provide one or multiple hard negatives per anchor-positive pair by structuring the data like this: (a_1, p_1, n_1), (a_2, p_2, n_2). Here, n_1 is a hard negative for (a_1, p_1). The loss will use for the pair (a_i, p_i) all p_j (j!=i) and all n_j as negatives.,But why not CosineSimilarityLoss? The most simple way is to have knowledge tuples annotated with a score indicating their similarity, e.g. on a scale of 0 to 1. We can then train the network with a Siamese Network Architecture. But we do not have a labeled score.
That the softmax-loss with NLI data produces (relatively) good sentence embeddings is rather coincidental. The MultipleNegativesRankingLoss is much more intuitive and produces also significantly better sentence representations.,The training data for MultipleNegativesRankingLoss consists of sentence pairs [(a1, b1), …, (an, bn)] where we assume that (ai, bi) are similar sentences and (ai, bj) are dissimilar sentences for i != j. The minimizes the distance between (ai, bj) while it simultaneously maximizes the distance (ai, bj) for all i != j.,For example in the following picture:,The distance between (a1, b1) is reduced, while the distance between (a1, b2…5) will be increased. The same is done for a2, …, a5.,Using MultipleNegativeRankingLoss with NLI is rather easy: We define sentences that have an entailment label as positive pairs. E.g, we have pairs like (“A soccer game with multiple males playing.”, “Some men are playing a sport.”) and want that these pairs are close in vector space.,The fine-tuning frame is ready to be fine-tuned. We have successfully established the model architecture and loss function and why to use them.,Once the tuning is done, do not forget to re-run the FAISS encoding and indexing on the dataset again with this model.,Once done i fired the similar queries to the new model and voilà !! The recommendations or the search results improved a lot. If you are not convinced go through the plots and you will understand.,This is half-battle won. We have just scratched the surface of the search. We still have not talked about the quality of retrieval,re-ranking, and personalization.,medium.com,In the next post, I will discuss in detail the strategy of re-ranking and improving search results with user behavior. Till then keep learning.
Please do comment with your thoughts and inputs.,Cheers!!,Youtube Link: https://youtu.be/Yo4NqGPISXQ,Note: All thanks to the SBERT paper and their Documentation.,github.com,Also if you’re looking for Fast-API based project skeleton for your ML/DL based project, do check out,github.com,www.linkedin.com,RedisAI vector Search demo — https://github.com/RedisAI/vecsim-demo,medium.com,🔵 Become a ML Writer,For symmetric semantic search, your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be “How to learn Python online?” and you want to find an entry like “How to learn Python on the web?”. For symmetric tasks, you could potentially flip the query and the entries in your corpus.,For asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like “What is Python” and you want to find the paragraph “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense.,paraphrase-distilroberta-base-v1 / paraphrase-xlm-r-multilingual-v1,quora-distilbert-base / quora-distilbert-multilingual,distiluse-base-multilingual-cased-v2,msmarco-distilbert-base-v2,msmarco-bert-base-v3,Release Year — Year in which the movie was released,Title — Movie title,Origin/Ethnicity — Origin of the movie (i.e. American, Bollywood, Tamil, etc.),Director — Director(s),Cast — Main actor and actresses,Genre — Movie Genre(s),Wiki Page — URL of the Wikipedia page from which the plot description was scraped,Plot — Long-form description of movie plot (WARNING: May contain spoilers!!!)"
Billion-scale semantic similarity search with FAISS+SBERT | by Mathew Alexander | Towards Data Science,https://towardsdatascience.com/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2,"Oct 19, 2020,Save,Semantic search is an information retrieval system that focuses on the meaning of the sentences rather than the conventional keyword matching. Even though there are many text embeddings that can be used for this purpose, scaling this up to build low latency APIs that can fetch data from a huge collection of data is something that is seldom discussed. In this article, I will discuss how we can implement a minimal semantic search engine using SOTA sentence embeddings (sentence transformer) and FAISS.,It is a framework or set of models that give dense vector representations of sentences or paragraphs. These models are transformer networks(BERT, RoBERTa, etc.) which are fine-tuned specifically for the task of Semantic textual similarity as the BERT doesn’t perform well out of the box for these tasks. Given below is the performance of different models in the STS benchmark,We can see that the Sentence transformer models outperform the other models by a large margin.,But if you look at the leaderboard by papers with code and GLUE, you would see many models above 90. So why do we need Sentence transformers?.,Well, In those models, the semantic Textual similarity is considered as a regression task. This means whenever we need to calculate the similarity score between two sentences, we need to pass them together into the model and the model outputs the numerical score between them. While this works well for the benchmarking test, it scales badly for a real-life use case, and here are the reasons.,Faiss is a C++ based library built by Facebook AI with a complete wrapper in python, to index vectorized data and to perform efficient searches on them. Faiss offers different indexes based on the following factors,So choosing the right index will be a trade-off between these factors.,First, let us install and load the required libraries,Loading the dataset with a million datapoints,I used a dataset from Kaggle that contains news headlines published over a period of seventeen years.,Loading the pre-trained model and performing the inference,We can choose different indexing options based on our use case by referring to the guide.,Let's define the index and add data to it,Serializing the index,The serialized index can be then exported into any machine for hosting the search engine,Deserializing the index,Let us first build a wrapper function for search,performing the search,Now let's take a look at the search results and response time,1.5 seconds is all it takes to perform an intelligent meaning-based search on a dataset of million text documents with just the CPU backend.,First, let's uninstall the CPU version of Faiss and reinstall the GPU version,Then follow the same procedure, but at the end move the index to GPU.,Now let's place this inside the search function and perform the search with the GPU.,That’s right, you can get the results within 0.02 sec with a GPU ( Tesla T4 is used in this experiment) which is 75 times faster than a CPU backend,Because the NumPy doesn’t come with native serialization functions, hence the only way is to convert it into a JSON and then save the JSON object, but then the size will increase by a factor of five. For example, a million data points encoded into a 768-dimensional vector space with normal indexing will be about 3GB, converting it into JSON will make it 15GB, which a normal machine can’t hold on it’s RAM. So we will have to run a million computational inference each time we perform a search, which is not practical.,This is a basic implementation and a lot needs to be still done both on the language model part and the indexing part. There are different indexing options from which the right one should be chosen based on the use case, size of the data and the compute power available. Also, the sentence embedding used here is just fine-tuned on some public datasets, fine-tuning them on the domain-specific dataset would improve the embeddings and hence the search results.,[1] Nils Reimers and Iryna Gurevych. “Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation.” arXiv (2020): 2004.09813.,[2]Johnson, Jeff and Douze, Matthijs and J{\’e}gou, Herv{\’e}. “Billion-scale similarity search with GPUs” arXiv preprint arXiv:1702.08734.,When you need to search over say 10 k documents, you would need to perform 10k separate inference computations, its not possible to compute the embeddings separately and calculate just the cosine similarity. See the author’s explanation.,The maximum sequence length (The total number of words/tokens the model can take at one pass) is shared between two documents, which causes the representations to be diluted due to chunking,search time,search quality,memory used per index vector,training time,need for external data for unsupervised training"
"Building a blog with SvelteKit, TailwindCSS, and MDsveX",https://jeffpohlmeyer.com/building-a-blog-with-sveltekit-tailwindcss-and-mdsvex,"Photo by Sincerely Media on Unsplash,18 min read,This post can also be found on my main page at jvp.design/blog/building-a-blog-with-svelte..,For those who want to follow along in code, you can get it from github.com/jvp-design/sveltekit-tailwind-md..,A YouTube video accompanying this article has been uploaded. Check it out: youtu.be/-OSTAkjGVng,In the last blog post I described how to self-host a font in SvelteKit using Tailwind CSS. I then tried to do a live-stream in which I used the aforementioned functionality and extended it by installing MDsveX and setting it up. I left off the video with a very trimmed down blog sample and noted that I may add styling if people wanted it. In this post I will not only add in styling but will also include things like pagination and search.,There were a few things that I did in the video that need to be updated from the last blog post.,The first thing that is different from the blog post is the fonts that were used. In the last blog post I used Walkway and Lobster Two and in the blog post I used Uncut Sans and Sprat. As such, the app.css file should replace the @font-face elements and look like,Here I'm using a few more fonts and I've also set it up where any header tags will automatically use the Sprat font by default. Then the tailwind.config.js file should now look like this,The next thing that needs to be done to approach where we were in the video is installing and setting up MDsveX. The first thing you need to do is install the package from npm,Then you'll need to create a file called mdsvex.config.js at the root of your project. I'll display the file in its entirety here and explain it below,The dirname variable effectively converts the ""current"" location of the project in your own file-system. This value is then used to point directly to the layout we'll be using for blog posts, called ""blog"", in the file system of the project. So, we're telling MDsveX that any file we set up with the template named ""blog"" should use the _layout.svelte file that we'll be creating and saving in the ./src/routes/blog/ folder. The extensions entry in the config just tells MDsveX to look convert files with either the .md or .svx extension. Information about smartypants functionality can be found at mdsvex.pngwn.io/docs#smartypants.,Now we need to tell our SvelteKit project to actually use this config that we've just set up. In svelte.config.js you'll need to add two things:,Your svelte.config.js file should now look like,Finally, we need to add the aforementioned _layout.svelte file to the /src/routes/blog folder. For now it will simply have a <slot /> element, but we'll add styling and functionality later.,First, let's create a few very simple markdown files to act as blog posts:,/src/routes/blog/foo-bar.md,/src/routes/blog/foo-baz.md,/src/routes/blog/hello-world.md,The way the latest version of SvelteKit works is that you no longer need to use <script context=""module""> and export an async load function, etc. We can very simply grab all the posts and return them as a body element from a JavaScript file, and then use the same key as a prop in the Svelte component. Before we can parse the files I want to introduce a simple function to convert a raw UTC date to a more human-friendly date time. This is fairly straightforward and I just include it here for reference.,We then use this function in the next file.,The first line fetches all files that are in /src/routes/blog that end with either an .svx or .md extension. The resulting allPostFiles is somewhat nonsensical for our purposes and looks like,What we can do, though, is cycle through each entry, and fetch the post path (the slice takes off the beginning ./ characters and ending .md characters) and grabs any metadata created by MDsveX. The metadata created by MDsveX is all the content in between the --- characters, also called ""front matter"" in the markdown file. We then simply sort the files descending by date (i.e. most recent first) and if there are no posts then we simply return a 404 error, otherwise we return the posts as the body of the response, with the key of posts.,We now need to create a file where we can view all the available blog posts. This file name needs to match the base (without the extension) of the JavaScript file we just created. Thus, we will create a /src/routes/blog/index.svelte file and add render the posts.,and if we navigate to http://localhost:3000/blog/ we should then see,Now that we've essentially gotten up-to-date relative to the last video, let's start styling things.,The first thing to do is remove the grid layout from the main /src/routes/__layout.svelte file because I want to be able to set the layout in the individual page. Now __layout.svelte looks like,Now we can update /src/routes/blog/index.svelte with a bit more pleasing styling,There is a lot to work through in all of the classes above and I encourage you to go to the Tailwind website to explore what each thing is doing, but you can see what the resulting image looks like here,We now want to update the styling for each blog post. The first thing we need to do is install another dependency because we'll be using some built-in Tailwind functionality. We can install the typography package by typing,and we then add this as a plugin in tailwind.config.js,Now that we actually have some blog posts we'll need to update our blog _layout.svelte file to be able to consume them. The first thing we're going to need to do is expose the props that we'll want to use in the post content, namely,and we'll also need to import convertDate to make a ""pretty"" date on the post. The script section of /src/routes/blog/_layout.svelte will then look like,Then we'll set certain display characteristics on the main content, such as centering, setting max widths, etc,We then add a back button to go to the previous page, and for this we'll also directly use the svg code from Heroicons for arrow-left.,Finally we'll add a bit of simple styling for the title, author/date combination, and the main content,Thus, now the whole _layout.svelte component looks like,and if we visit http://localhost:3000/blog/foo-baz then we're greeted by the following image,There are two main things that can be helpful in what we're doing,To demonstrate this functionality we will do a search by author. So, go ahead and change the author of any of the already existing blog posts (I'll be doing this on foo-baz.md) to any other name (I'm using ""Alice""). Next we need to go into /src/routes/blog/index.js and look for this search functionality. The first step is to add an argument called {url} to the main get function. Now the function signature looks like export async function get({ url }) {,We can next add in a filter to check the author, if one is passed in from the url argument.,In the last line of the ""New Code"" block, I've simply replaced the array on which I'm sorting from allPosts to authorPosts. The way this works is that if there is no author parameter included in the search then just return all posts, otherwise only return posts where the author matches the author term queried for.,Next we need to add functionality where a user can search by author from the main blog list view. This is done very simply in that we just need to replace the p tag that displays the author name in /src/routes/blog/index.svelte with an a tag and set the href attribute to be ?author={post.author}. The updated component looks like,This functionality can be extended to any number of search parameters as well as things like pagination, but that will be handled another time.,One problem that I had when I tried using ""simple"" CMSes was that any time I wanted to include a URL it would open in the current page. This defeats one of the goals of a blog as it pertains to SEO: the amount of time spent on your site. Thus, ultimately, it would be beneficial to be able to have a way to tell SvelteKit to open URLs in a new page. We can do this by creating a custom Svelte component that we can use in our markdown files.,This will be a fairly simple component in that all we need it to render is a pre-defined url with a description and have the target=""_blank"" attribute on it.,Now we can use this in one of our blog posts (markdown files). Let's add a link to the root route of my site, but have it open in a new page. We'll create a new blog post: /src/routes/blog/test-custom-url.md,Then if we visit http://localhost:3000/blog/test-custom-url and click on the link it should open my main site on a new page.,We can use this for any number of things. For example, for the blog posts for this site I have a custom component to handle different image formats (.webp and .png) which use the picture tag with a source tag for each item in an image's srcset group. Maybe that can be another (shorter) blog post.,Note: This post can also be found on my main page at https://www.jvp.design/blog/self-hosting-a-font…,As I mentioned in my last post, what I've been doing with the bulk of whatever free time I have over…,So I had started this blog back in November (I actually joined in August) with the intent of buildin…,Kai Pereira,Crazy Coder 😎,Jun 3,Wow! I remember doing all of that with a blog I was doing for someone using the Netlify CMS! It is pretty cool!,Notes,Intro,Updates to the last blog post
Different fonts
MDsveX
Installation
Configuring
Adding to SvelteKit Project
Adding a Blog Layout
Fetching Blog Posts
Sample Blog Posts
Fetch the Blog Posts for Svelte
Create ""List View"",Different fonts,MDsveX
Installation
Configuring
Adding to SvelteKit Project
Adding a Blog Layout,Installation,Configuring,Adding to SvelteKit Project,Adding a Blog Layout,Fetching Blog Posts
Sample Blog Posts
Fetch the Blog Posts for Svelte
Create ""List View"",Sample Blog Posts,Fetch the Blog Posts for Svelte,Create ""List View"",New ""Stuff""
Blog List View
Blog ""Detail View""
Update /src/routes/blog/_layout.svelte
Extra Functionality
Search
Custom Svelte Components
NewWindowUrl Component,Blog List View,Blog ""Detail View""
Update /src/routes/blog/_layout.svelte,Update /src/routes/blog/_layout.svelte,Extra Functionality
Search
Custom Svelte Components
NewWindowUrl Component,Search,Custom Svelte Components
NewWindowUrl Component,NewWindowUrl Component,extensions: ['.svelte', ...mdsvexConfig.extensions],mdsvex(mdsvexConfig) to the preprocess attribute,title,author,date,Search,Custom Svelte components in markdown"
Introduction to git and github,https://ezefizzy.hashnode.dev/introduction-to-git-and-github,"4 min read,Outlines,When you are working on a simple project, such as a single page html, it is very easy to remember the last thing you changed and where the changed occurred.,Version Control Systems (VCS) help a software team manage changes to source code over time. VCS software includes tools for saving the state of a project, viewing the history of changes, and reverting changes. Developing software without using version control is risky, similar to not having backups. VCS can also enhance and speed up development. Depending on the version control software used, many developers can work on the same code at the same time.,There are two types of version control.,All the team members or people working on the same code connect to the central server to get the latest copy of the code, and to share their changes with others. The pitfall of centralized VCS is if the server goes offline, we can’t collaborate and save snapshots of our project. So, we have to wait until the project comes back online.,we don’t have this problem. Every team member has a copy of the project with its history on their machine. So, we can save the snapshots of our project locally on our computer. If our server is offline, we can synchronize our work directly with others. One of the most popular version control systems is Git, a distributed VCS.,Git has another advantage - it is distributed. Rather than having only one single place for the full version history of a project, every developer's working copy of the code is also a repository that can contain the full history of all changes. Git was created by Linus Torvalds in 2005 for development of the Linux kernel, with other kernel developers contributing to its initial development.,GitHub is the cloud-hosted Git management tool and git is Version Control System. Github helps us to share and store our code easily.,Github helps us to share and store our code easily. With the help of github we can work on project irrespective of our location or schedule.,The steps below illustrate how to create a github account;,lastly login to your github account using your registered username and password Boom you now have a github Account.,Before we can monitor or control our project, we need to have git install and configure on our local machine.,You can download git from here and for installation, use these instructions provided by git (if it’s not already installed).,if it's your first time using git then you need to configure it, so that git can identify the user. follow the steps below to configure git on your local machine,Steps to configure git on your local machine(computer),Search for command prompt on window and command Line mac then open program to lunch the command interface,on your command line run the command below to configure git on your machine:,So far, we have been able to cover some basics of git and github, in our next article we are going to be covering How to push our project to github. Please do comment, share and like, see you in the next article.,I'm Ayebidun Ezekiel. A fullstack developer with over 5 years of experience and very passionate abou…,Coding Kiwi,#100DaysOfWeb3 | Defi | Blogging My Tech Journey,17 hrs ago,Thanks for sharing. As a beginner I wanted to know more about GitHub so this definitely help!,Introduction to git,Types of version control,Introduction to github,Why use github,How to create a github account,How to install and configure git,Centralized version control system,Distributed control system,Load GitHub website on your browser or click GitHub,Create a new account by clicking the sign up button and fill up necessary information such as username, email password etc.,Search for command prompt on window and command Line mac then open program to lunch the command interface,on your command line run the command below to configure git on your machine:"
Monorepo. Building one roof for your UI apps.,https://sagarpreet.in/monorepo-building-one-roof-for-your-ui-apps,"9 min read,Hi there,,I recently merged all the repos (even express backend server code) into a single repository and wanted to share my learnings with code samples with you all...,Prerequisites: You should have coffee or tea or even beer in your vicinity.,Monorepos are there in existence from many years - the reasons for any frontend/web team to opt for monorepo maybe different but boils down to mainly these 3 points:,I guess you would have all guessed from the title that this article is going to be about - Monorepo with turborepo,There are many tools available in the market, listing the most popular ones here:,Yes there are many tools and you should choose the tool which best suits your requirement. Here's an image from this excellent website - monorepo.tools which extensively compares all these tools in depth.,Main selling point is the integration - it is very simple and easy to understand especially for a new person joining your team - the various apps workflows and their inter-dependencies can be understood easily. Also Turborepo has a great community support and many amazing things seems to be lined up in this project by the Vercel team.,Turborepo easily finds the pruned subset of your monorepo to quickly build only your target web app without installing modules of other web apps.,Jump to last section to see how to make Dockerfile using turbo prune command.,More Control - You can only build the web app that has code changes without building any of it's dependent. Or you can choose to build the dependents as well. More details on --no-deps here.,We will take a look at the basic project structure first then cover following topics:,Let's have look at the folder structure:,Let's take a closer look at /apps folder:,Let's take a closer look at /packages folder:,Root package.json mainly contains:,NOTE 1: yarn workplaces has a concept of hoisting, where common packages between 2 sibling apps are hoisted to their parent's node_module.,NOTE 2: when searching for a package in a app, first app's own node_module is searched then it's parent, then parent's parent and so on..,lint-staged and prettier setup can be done exactly the same way !!,That's it - now both frontend apps are powered by tailwind css.,NOTE: TailwindCSS autosuggestion will not work now as there are multiple tailwind.config.js files in the repo. To make it work in VS code editor, add these lines to .vscode/settings.json:,Depending upon your need, you can define the dependency graph of your monorep like what package to build first, what package server to start first OR what task to execute inside a package and it's dependent. Example: build task in your package may depend on test, tsc and formatting/prettier tasks. Another Example: you may want to deploy your app-1 first then only deploy your app-2.,Turbo interprets this configuration to optimally schedule, execute, and cache the outputs of each of the package.json scripts defined in your workspaces. More here.,Prerequisite: Refill your tea/coffee/beer glasses if empty,Now the most interesting part - how do we prune all the unwanted packages and keep which is only relevant to the target app - let's say frontend-app-1?,Here's the Dockerfile in action, we will go through each stage in detail:,Example of pruned monorepo:,out/json folder looks like:,out/full contains the same structure as out/json folder but with full code !!,Each command that is found in a Dockerfile creates a new layer. Each layer contains the filesystem changes to the image for the state before the execution of the command and the state after the execution of the command. If the file system changes are present then cached layer is ignored!,Alright! You have reached the end of this article. If you liked this article, kindly give likes and comment the best part you liked about this article.,Mohd Shad Mirza,Developer, Hashnode,9 hrs ago,Great read. Thanks for sharing,Markos Korvesis,Backend Software Engineer,8 hrs ago,Sagarpreet Chadha, you've certainly improved my opinion towards mono-repos; pior to reading this I'd definitely say ""no"", but now I might consider it.,On the other hand, I believe it should be approached with caution as it's purely situational on how the projects are internally structured and the size of the codebase.,I've worked on a mono-repo in my youth and I can safely say that it was purely chaotic as per navigation, not to mention the horrific loading times I faced every time I needed to startup my IDE, search, etc.,Diego Ballesteros (Relatable Code),Senior Software Developer (Relatable Code),14 mins,I've seen more and more articles that talk about Turborepo. Will definitely have to play around with it some to get a better opinion.,Thanks for sharing.,Re-usability of common code, configurations, components, etc.,Ease of new project setup:
[Basic configurations] Should have all the configs like eslint, prettier, .vscode settings, husky + lint-staged, tsconfig etc. should be done in under ~1 minute
[Advance configurations] Tools like jest config, MSW and tailwindcss setup should be done in under ~5 minutes,[Basic configurations] Should have all the configs like eslint, prettier, .vscode settings, husky + lint-staged, tsconfig etc. should be done in under ~1 minute,[Advance configurations] Tools like jest config, MSW and tailwindcss setup should be done in under ~5 minutes,Better Developer Experience
No explicit yarn linking of different packages OR switching between repos while doing development.
Ease in team workflow: Merge all your changes related to your task in 1 go, instead of merging each PR for different repo separately (This maybe a real pain in your team too - let's say express backend code might merge before, causing the UI code to break for other team mates until that developer's UI code is also merged).,No explicit yarn linking of different packages OR switching between repos while doing development.,Ease in team workflow: Merge all your changes related to your task in 1 go, instead of merging each PR for different repo separately (This maybe a real pain in your team too - let's say express backend code might merge before, causing the UI code to break for other team mates until that developer's UI code is also merged).,Bazel,Nx,Gradle,Pants,Lerna,Turborepo,Rush,Turborepo easily finds the pruned subset of your monorepo to quickly build only your target web app without installing modules of other web apps.

Jump to last section to see how to make Dockerfile using turbo prune command.,More Control - You can only build the web app that has code changes without building any of it's dependent. Or you can choose to build the dependents as well. More details on --no-deps here.,Root package.json,Adding Basic configurations: eslint, prettier, lint-staged + husky, tsconfig,Adding Advance configurations: tailwindcss, jest,turbo.json to add workflow settings,Create Dockerfile using turbo prune,It contains 2 apps: frontend-app-1 and frontend-app-2 powered by NextJS.,It contains all the pluggable config packages that can be easily added to frontend-app-1 and frontend-app-2,workspaces: regex for yarn workspaces to find your various apps and packages,Scripts to turbo charge your project, turbo will look for package.json of frontend-app-1 and frontend-app-2 and execute the scripts with same name in their package.json (if present) in the most optimised way possible.,Create a packages/config folder, which will have configs for eslint, lint-staged and prettier,The packages/config/package.json is required to make it a package which can be added to package.json's of frontend-app-1 and frontend-app-2,packages/config/package.json file looks like:,packages/config/eslint-preset.js file looks like:,Finally, we have our first independent package ready, let's add config package to package.json's of frontend-app-1 and frontend-app-2,Add following line to apps/frontend-app-1/package.json and apps/frontend-app-2/package.json:,Last but not the least, let's make .eslintrc.js in frontend-app-1 and frontend-app-2 with following code:,Let's start with adding tailwindcss to both frontend apps:,Step 1 is to create a new package under /packages folder called tailwind-config,Let's add tailwind.config.js, postcss.config.js and styles folder (see tailwind setup docs for more details) in tailwind-config folder,Create a package.json in packages/tailwind-config:,Add tailwind to root package.json as tailwind is used by both frontend apps:,You are all set now to add tailwind to your projects now !!,Add following code to apps/frontend-app-1/package.json and apps/frontend-app-2/package.json:,Create and add following code to apps/frontend-app-1/tailwind.config.js and apps/frontend-app-2/tailwind.config.js:,Create and add following code to apps/frontend-app-1/postcss.config.js and apps/frontend-app-2/postcss.config.js:,Add Tailwind styles in apps/frontend-app-1/styles/index.css and apps/frontend-app-2/styles/index.css:,This file is used by Turborepo to create a pipeline.,Depending upon your need, you can define the dependency graph of your monorep like what package to build first, what package server to start first OR what task to execute inside a package and it's dependent. Example: build task in your package may depend on test, tsc and formatting/prettier tasks. Another Example: you may want to deploy your app-1 first then only deploy your app-2.,Turbo interprets this configuration to optimally schedule, execute, and cache the outputs of each of the package.json scripts defined in your workspaces. More here.,Build you project once via yarn build:,Build you project the second time (with no code changes after step1):,This stage globally installs turbo and using it's prune command - prunes all unwanted packages and outputs a subset of your monorepo with only frontend-app-1 and it's dependent packages!,This stage copies only the content of out/json folder with newly generated pruned yarn.lock and does yarn install.,As package.json does not changes frequently, it makes more sense to copy only the out/json folder which has only package.json of apps and packages folder. This stage is then cached and will only re-run freshly if any of the package.json is changed.,If instead you copy out/full folder, then this stage will be executed everytime (as your code files will have new changes).

Each command that is found in a Dockerfile creates a new layer. Each layer contains the filesystem changes to the image for the state before the execution of the command and the state after the execution of the command. If the file system changes are present then cached layer is ignored!,Now from stage-2, you got your node_modules required for building your app frontend-app-1,Copy the node_modules from stage-2 and the full code of your app from stage-1, and finally run turbo build to build your app.,Here we are mainly copying the files relevant for running the server,This is also done so that the docker image size is less"
